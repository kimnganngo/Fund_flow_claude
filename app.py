import streamlit as st
import pandas as pd
import networkx as nx
from pyvis.network import Network
import plotly.graph_objects as go
from unidecode import unidecode
import re
from io import BytesIO
import base64
from datetime import datetime
import community.community_louvain as community_louvain

# ==================== CHU·∫®N H√ìA D·ªÆ LI·ªÜU ====================

def normalize_name(name: str) -> str:
    """Chu·∫©n h√≥a t√™n ng∆∞·ªùi: b·ªè d·∫•u, vi·∫øt hoa, g·ªôp kho·∫£ng tr·∫Øng"""
    if pd.isna(name):
        return ''
    name = str(name).strip()
    name = unidecode(name)
    name = re.sub(r'\s+', ' ', name)
    name = name.upper()
    return name

def normalize_account(acc: str) -> str:
    """Chu·∫©n h√≥a m√£ t√†i kho·∫£n: b·ªè kho·∫£ng tr·∫Øng, vi·∫øt hoa"""
    if pd.isna(acc):
        return ''
    acc = re.sub(r'\s+', '', str(acc).strip()).upper()
    return acc

def parse_amount(x):
    """Chuy·ªÉn ƒë·ªïi s·ªë ti·ªÅn t·ª´ d·∫°ng string c√≥ d·∫•u ch·∫•m/ph·∫©y"""
    if pd.isna(x):
        return 0
    s = str(x).replace('.', '').replace(',', '')
    s = re.sub(r'[^\d\-]', '', s)
    try:
        return int(s)
    except:
        try:
            return float(s)
        except:
            return 0

# ==================== X·ª¨ L√ù D·ªÆ LI·ªÜU ====================

def load_and_process_data(flow_file, group_file=None, exclude_internal=True, transaction_type='C·∫£ hai'):
    """ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ 2 file"""
    
    # ƒê·ªçc file lu·ªìng ti·ªÅn
    if flow_file.name.endswith('.csv'):
        df_flow = pd.read_csv(flow_file)
    else:
        df_flow = pd.read_excel(flow_file)
    
    # Chu·∫©n h√≥a c·ªôt
    df_flow['Nguoi_nop_norm'] = df_flow['Nguoi_nop'].apply(normalize_name)
    df_flow['Tai_khoan_norm'] = df_flow['Tai_khoan'].apply(normalize_account)
    df_flow['Ten_nha_dau_tu_norm'] = df_flow['Ten_nha_dau_tu'].apply(normalize_name)
    df_flow['So_tien_clean'] = df_flow['So_tien'].apply(parse_amount)
    
    # L·ªçc theo t√πy ch·ªçn
    if exclude_internal:
        df_flow = df_flow[df_flow['Tu_chuyen_khoan'] != True]
    
    if transaction_type == 'N·ªôp':
        df_flow = df_flow[df_flow['NoP_Rut'] == 'N·ªôp']
    elif transaction_type == 'R√∫t':
        df_flow = df_flow[df_flow['NoP_Rut'] == 'R√∫t']
    
    # ƒê·ªçc file nh√≥m danh t√≠nh (n·∫øu c√≥)
    df_group = None
    if group_file is not None:
        if group_file.name.endswith('.csv'):
            df_group = pd.read_csv(group_file)
        else:
            df_group = pd.read_excel(group_file)
        
        df_group['Ma_TK_norm'] = df_group['Ma_TK'].apply(normalize_account)
        df_group['Ten_norm'] = df_group['Ten'].apply(normalize_name)
    
    # Gh√©p d·ªØ li·ªáu
    if df_group is not None:
        df_merged = df_flow.merge(
            df_group[['Ma_TK_norm', 'STT_nhom', 'Ten', 'Moi_quan_he_voi_cong_ty']],
            left_on='Tai_khoan_norm',
            right_on='Ma_TK_norm',
            how='left'
        )
    else:
        df_merged = df_flow.copy()
        df_merged['STT_nhom'] = None
        df_merged['Ten'] = df_merged['Ten_nha_dau_tu']
        df_merged['Moi_quan_he_voi_cong_ty'] = 'Unknown'
    
    return df_merged

def create_edges(df, min_amount=0):
    """T·∫°o danh s√°ch c·∫°nh t·ª´ d·ªØ li·ªáu"""
    
    # L·ªçc theo ng∆∞·ª°ng ti·ªÅn
    df_filtered = df[df['So_tien_clean'] >= min_amount]
    
    # G·ªôp theo ngu·ªìn - ƒë√≠ch
    edges = df_filtered.groupby([
        'Nguoi_nop_norm', 
        'Tai_khoan_norm',
        'NoP_Rut',
        'STT_nhom'
    ]).agg({
        'So_tien_clean': 'sum',
        'So_lenh': 'sum',
        'Nguoi_nop': 'first',
        'Ten_nha_dau_tu': 'first',
        'Moi_quan_he_voi_cong_ty': 'first'
    }).reset_index()
    
    edges.columns = ['Nguon_norm', 'Dich_norm', 'Loai', 'STT_nhom', 
                     'Tong_tien', 'So_lenh', 'Nguon', 'Ten_dich', 'Moi_quan_he']
    
    return edges

# ==================== GRAPH ANALYSIS ====================

def build_graph(edges_df):
    """X√¢y d·ª±ng ƒë·ªì th·ªã t·ª´ edges"""
    G = nx.DiGraph()
    
    for _, row in edges_df.iterrows():
        G.add_edge(
            row['Nguon_norm'],
            row['Dich_norm'],
            weight=row['Tong_tien'],
            orders=row['So_lenh'],
            group=row['STT_nhom'],
            relation=row['Moi_quan_he']
        )
        
        # Th√™m thu·ªôc t√≠nh cho node
        G.nodes[row['Nguon_norm']]['type'] = 'NGUON'
        G.nodes[row['Nguon_norm']]['label'] = row['Nguon']
        G.nodes[row['Nguon_norm']]['group'] = None
        
        G.nodes[row['Dich_norm']]['type'] = 'TAIKHOAN'
        G.nodes[row['Dich_norm']]['label'] = row['Ten_dich']
        G.nodes[row['Dich_norm']]['group'] = row['STT_nhom']
        G.nodes[row['Dich_norm']]['relation'] = row['Moi_quan_he']
    
    return G

def calculate_metrics(G):
    """T√≠nh c√°c ch·ªâ s·ªë graph"""
    metrics = {}
    
    # Degree centrality
    in_degree = dict(G.in_degree(weight='weight'))
    out_degree = dict(G.out_degree(weight='weight'))
    
    # Betweenness centrality
    betweenness = nx.betweenness_centrality(G, weight='weight')
    
    # Community detection
    G_undirected = G.to_undirected()
    communities = community_louvain.best_partition(G_undirected)
    
    for node in G.nodes():
        metrics[node] = {
            'in_degree': in_degree.get(node, 0),
            'out_degree': out_degree.get(node, 0),
            'betweenness': betweenness.get(node, 0),
            'community': communities.get(node, 0)
        }
    
    return metrics

def calculate_risk_score(row, edges_df):
    """T√≠nh risk score cho m·ªói c·∫£nh b√°o"""
    score = 0
    
    # +3: Ngu·ªìn c·∫•p cho ‚â•3 t√†i kho·∫£n kh√°c nh√≥m
    if row.get('Loai_canh_bao') == 'Cross-group':
        unique_groups = edges_df[edges_df['Nguon_norm'] == row.get('Nguon_norm', '')]['STT_nhom'].nunique()
        if unique_groups >= 3:
            score += 3
    
    # +2: Quan h·ªá n·ªôi b·ªô/CƒêL/NBTT
    sensitive_relations = ['NBTT', 'CƒêL', 'n·ªôi b·ªô', 'ng∆∞·ªùi n·ªôi b·ªô', 'c·ªï ƒë√¥ng l·ªõn']
    if any(rel in str(row.get('Moi_quan_he', '')).lower() for rel in [r.lower() for r in sensitive_relations]):
        score += 2
    
    # +1: T·ªïng ti·ªÅn ‚â• 5 t·ª∑
    if row.get('Tong_tien', 0) >= 5_000_000_000:
        score += 1
    
    return min(score, 10)  # Cap at 10

# ==================== PH√ÅT HI·ªÜN C·∫¢NH B√ÅO ====================

def detect_alerts(edges_df, G, metrics):
    """Ph√°t hi·ªán c√°c tr∆∞·ªùng h·ª£p c·∫£nh b√°o"""
    alerts = []
    
    # 1. C√πng ngu·ªìn trong c√πng nh√≥m
    same_group = edges_df[edges_df['STT_nhom'].notna()].groupby(['Nguon_norm', 'STT_nhom']).agg({
        'Dich_norm': lambda x: list(x),
        'Tong_tien': 'sum',
        'So_lenh': 'sum',
        'Nguon': 'first',
        'Moi_quan_he': 'first'
    }).reset_index()
    
    same_group = same_group[same_group['Dich_norm'].apply(len) >= 2]
    
    for _, row in same_group.iterrows():
        alert_data = {
            'Loai_canh_bao': 'Same-group',
            'Nguon': row['Nguon'],
            'Nguon_norm': row['Nguon_norm'],
            'STT_nhom': row['STT_nhom'],
            'So_tai_khoan': len(row['Dich_norm']),
            'Tong_tien': row['Tong_tien'],
            'So_lenh': row['So_lenh'],
            'Moi_quan_he': row['Moi_quan_he'],
            'Chi_tiet': f"Ngu·ªìn c·∫•p ti·ªÅn cho {len(row['Dich_norm'])} TK c√πng nh√≥m {row['STT_nhom']}"
        }
        alert_data['Risk_Score'] = calculate_risk_score(alert_data, edges_df)
        alerts.append(alert_data)
    
    # 2. Cross-group (c√πng ngu·ªìn kh√°c nh√≥m)
    cross_group = edges_df[edges_df['STT_nhom'].notna()].groupby('Nguon_norm').agg({
        'STT_nhom': lambda x: list(x.unique()),
        'Dich_norm': lambda x: list(x),
        'Tong_tien': 'sum',
        'So_lenh': 'sum',
        'Nguon': 'first',
        'Moi_quan_he': 'first'
    }).reset_index()
    
    cross_group = cross_group[cross_group['STT_nhom'].apply(lambda x: len(x) >= 2)]
    
    for _, row in cross_group.iterrows():
        alert_data = {
            'Loai_canh_bao': 'Cross-group',
            'Nguon': row['Nguon'],
            'Nguon_norm': row['Nguon_norm'],
            'STT_nhom': f"Xuy√™n {len(row['STT_nhom'])} nh√≥m",
            'So_tai_khoan': len(row['Dich_norm']),
            'Tong_tien': row['Tong_tien'],
            'So_lenh': row['So_lenh'],
            'Moi_quan_he': row['Moi_quan_he'],
            'Chi_tiet': f"‚ö†Ô∏è C·∫¢NH B√ÅO: Ngu·ªìn c·∫•p cho {len(row['Dich_norm'])} TK thu·ªôc {len(row['STT_nhom'])} nh√≥m kh√°c nhau: {row['STT_nhom']}"
        }
        alert_data['Risk_Score'] = calculate_risk_score(alert_data, edges_df)
        alerts.append(alert_data)
    
    # 3. T√†i kho·∫£n trung gian (high betweenness)
    high_betweenness = [(node, metrics[node]['betweenness']) 
                        for node in G.nodes() 
                        if G.nodes[node]['type'] == 'TAIKHOAN' 
                        and metrics[node]['betweenness'] > 0.01]
    
    high_betweenness.sort(key=lambda x: x[1], reverse=True)
    
    for node, score in high_betweenness[:10]:
        alert_data = {
            'Loai_canh_bao': 'Trung gian',
            'Nguon': G.nodes[node]['label'],
            'Nguon_norm': node,
            'STT_nhom': G.nodes[node].get('group'),
            'So_tai_khoan': G.out_degree(node),
            'Tong_tien': metrics[node]['in_degree'],
            'So_lenh': 0,
            'Moi_quan_he': G.nodes[node].get('relation'),
            'Chi_tiet': f"TK trung gian c√≥ betweenness = {score:.4f}"
        }
        alert_data['Risk_Score'] = calculate_risk_score(alert_data, edges_df)
        alerts.append(alert_data)
    
    return pd.DataFrame(alerts)

# ==================== VISUALIZATION ====================

def create_pyvis_graph(G, metrics, output_file='graph.html'):
    """T·∫°o graph t∆∞∆°ng t√°c v·ªõi PyVis"""
    net = Network(height='750px', width='100%', directed=True, notebook=False)
    
    # M√†u cho nh√≥m
    group_colors = {
        None: '#CCCCCC',
        1: '#FF6B6B',
        2: '#4ECDC4',
        3: '#45B7D1',
        4: '#FFA07A',
        5: '#98D8C8',
        6: '#F7DC6F',
        7: '#BB8FCE',
        8: '#85C1E2',
    }
    
    # Th√™m nodes
    for node in G.nodes():
        node_type = G.nodes[node]['type']
        label = G.nodes[node]['label']
        group = G.nodes[node].get('group')
        
        size = 10 + metrics[node]['in_degree'] / 1_000_000_000 * 5
        
        color = group_colors.get(group, '#CCCCCC')
        shape = 'dot' if node_type == 'TAIKHOAN' else 'square'
        
        title = f"""
        <b>{label}</b><br>
        Lo·∫°i: {node_type}<br>
        Nh√≥m: {group if group else 'N/A'}<br>
        Ti·ªÅn v√†o: {metrics[node]['in_degree']:,.0f} VNƒê<br>
        Ti·ªÅn ra: {metrics[node]['out_degree']:,.0f} VNƒê<br>
        Betweenness: {metrics[node]['betweenness']:.4f}<br>
        Community: {metrics[node]['community']}
        """
        
        net.add_node(node, label=label[:20], title=title, size=size, 
                     color=color, shape=shape)
    
    # Th√™m edges
    for edge in G.edges(data=True):
        weight = edge[2]['weight']
        width = 1 + weight / 10_000_000_000 * 5
        
        title = f"T·ªïng ti·ªÅn: {weight:,.0f} VNƒê<br>S·ªë l·ªánh: {edge[2]['orders']}"
        
        net.add_edge(edge[0], edge[1], value=width, title=title)
    
    # C·∫•u h√¨nh physics
    net.set_options("""
    {
      "physics": {
        "forceAtlas2Based": {
          "gravitationalConstant": -50,
          "centralGravity": 0.01,
          "springLength": 200,
          "springConstant": 0.08
        },
        "maxVelocity": 50,
        "solver": "forceAtlas2Based",
        "timestep": 0.35,
        "stabilization": {"iterations": 150}
      }
    }
    """)
    
    net.save_graph(output_file)
    return output_file

def create_plotly_graph(G, metrics):
    """T·∫°o graph v·ªõi Plotly"""
    pos = nx.spring_layout(G, k=2, iterations=50)
    
    edge_trace = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_trace.append(
            go.Scatter(
                x=[x0, x1, None],
                y=[y0, y1, None],
                mode='lines',
                line=dict(width=0.5, color='#888'),
                hoverinfo='none'
            )
        )
    
    node_trace = go.Scatter(
        x=[],
        y=[],
        text=[],
        mode='markers+text',
        hoverinfo='text',
        marker=dict(
            showscale=True,
            colorscale='YlGnBu',
            size=[],
            color=[],
            colorbar=dict(
                thickness=15,
                title='T·ªïng ti·ªÅn (t·ª∑)',
                xanchor='left',
                titleside='right'
            )
        )
    )
    
    for node in G.nodes():
        x, y = pos[node]
        node_trace['x'] += tuple([x])
        node_trace['y'] += tuple([y])
        node_trace['text'] += tuple([G.nodes[node]['label'][:15]])
        node_trace['marker']['size'] += tuple([10 + metrics[node]['in_degree'] / 1_000_000_000 * 5])
        node_trace['marker']['color'] += tuple([metrics[node]['in_degree'] / 1_000_000_000])
    
    fig = go.Figure(data=edge_trace + [node_trace],
                    layout=go.Layout(
                        title='M·∫°ng quan h·ªá lu·ªìng ti·ªÅn',
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=0, l=0, r=0, t=40),
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        height=700
                    ))
    
    return fig

# ==================== EXPORT FUNCTIONS ====================

def export_to_excel(edges_df, alerts_df, metrics_dict, G):
    """Xu·∫•t d·ªØ li·ªáu ra Excel nhi·ªÅu sheet"""
    output = BytesIO()
    
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        # Sheet 1: Edges
        edges_df.to_excel(writer, sheet_name='Edges', index=False)
        
        # Sheet 2: Alerts v·ªõi Risk Score
        if not alerts_df.empty:
            alerts_export = alerts_df.sort_values('Risk_Score', ascending=False)
            alerts_export.to_excel(writer, sheet_name='Alerts', index=False)
        
        # Sheet 3: Node metrics
        nodes_data = []
        for node in G.nodes():
            nodes_data.append({
                'Node_ID': node,
                'Label': G.nodes[node]['label'],
                'Type': G.nodes[node]['type'],
                'Group': G.nodes[node].get('group'),
                'In_Degree': metrics_dict[node]['in_degree'],
                'Out_Degree': metrics_dict[node]['out_degree'],
                'Betweenness': metrics_dict[node]['betweenness'],
                'Community': metrics_dict[node]['community']
            })
        
        pd.DataFrame(nodes_data).to_excel(writer, sheet_name='Nodes', index=False)
        
        # Sheet 4: Summary statistics
        summary = {
            'Metric': ['Total Nodes', 'Total Edges', 'Total Amount (VNƒê)', 
                      'Number of Communities', 'Number of Alerts'],
            'Value': [
                G.number_of_nodes(),
                G.number_of_edges(),
                edges_df['Tong_tien'].sum(),
                len(set(metrics_dict[n]['community'] for n in G.nodes())),
                len(alerts_df)
            ]
        }
        pd.DataFrame(summary).to_excel(writer, sheet_name='Summary', index=False)
    
    output.seek(0)
    return output

# ==================== STREAMLIT UI ====================

def main():
    st.set_page_config(page_title="Money Flow Analyzer", layout="wide", page_icon="üí∞")
    
    st.title("üîç C√¥ng c·ª• Ph√¢n t√≠ch Lu·ªìng Ti·ªÅn & Quan h·ªá T√†i kho·∫£n")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        st.subheader("üìÇ Upload d·ªØ li·ªáu")
        flow_file = st.file_uploader("File lu·ªìng ti·ªÅn (CSV/Excel)", 
                                     type=['csv', 'xlsx'],
                                     help="File ch·ª©a: Nguoi_nop, Tai_khoan, So_tien, NoP_Rut...")
        
        group_file = st.file_uploader("File nh√≥m danh t√≠nh (t√πy ch·ªçn)", 
                                      type=['csv', 'xlsx'],
                                      help="File ch·ª©a: STT_nhom, Ma_TK, Ten, Moi_quan_he...")
        
        st.markdown("---")
        st.subheader("üîß B·ªô l·ªçc")
        
        exclude_internal = st.checkbox("Lo·∫°i tr·ª´ giao d·ªãch n·ªôi b·ªô", value=True,
                                       help="Lo·∫°i b·ªè d√≤ng Tu_chuyen_khoan = TRUE")
        
        transaction_type = st.selectbox("Lo·∫°i giao d·ªãch", 
                                        ['C·∫£ hai', 'N·ªôp', 'R√∫t'])
        
        min_amount = st.number_input("Ng∆∞·ª°ng ti·ªÅn t·ªëi thi·ªÉu (VNƒê)", 
                                     min_value=0, 
                                     value=0, 
                                     step=100000000,
                                     format="%d")
        
        st.markdown("---")
        st.subheader("üìä T√πy ch·ªçn hi·ªÉn th·ªã")
        viz_type = st.radio("Lo·∫°i bi·ªÉu ƒë·ªì", ['PyVis (Interactive)', 'Plotly'])
    
    # Main content
    if flow_file is not None:
        try:
            # Load data
            with st.spinner("ƒêang x·ª≠ l√Ω d·ªØ li·ªáu..."):
                df = load_and_process_data(flow_file, group_file, exclude_internal, transaction_type)
                edges_df = create_edges(df, min_amount)
                
                if len(edges_df) == 0:
                    st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu sau khi l·ªçc. Vui l√≤ng ƒëi·ªÅu ch·ªânh b·ªô l·ªçc.")
                    return
                
                G = build_graph(edges_df)
                metrics = calculate_metrics(G)
                alerts_df = detect_alerts(edges_df, G, metrics)
            
            st.success(f"‚úÖ ƒê√£ t·∫£i {len(df)} giao d·ªãch, t·∫°o {G.number_of_nodes()} nodes v√† {G.number_of_edges()} edges")
            
            # Tabs
            tab1, tab2, tab3, tab4 = st.tabs(["üìà Bi·ªÉu ƒë·ªì", "‚ö†Ô∏è C·∫£nh b√°o", "üìä Th·ªëng k√™", "üíæ Xu·∫•t d·ªØ li·ªáu"])
            
            with tab1:
                st.subheader("M·∫°ng quan h·ªá lu·ªìng ti·ªÅn")
                
                if viz_type == 'PyVis (Interactive)':
                    html_file = create_pyvis_graph(G, metrics, 'temp_graph.html')
                    with open(html_file, 'r', encoding='utf-8') as f:
                        html_content = f.read()
                    st.components.v1.html(html_content, height=800, scrolling=True)
                    
                    # Download HTML
                    st.download_button(
                        label="üì• T·∫£i bi·ªÉu ƒë·ªì HTML",
                        data=html_content,
                        file_name=f"graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                        mime="text/html"
                    )
                else:
                    fig = create_plotly_graph(G, metrics)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Download PNG
                    img_bytes = fig.to_image(format="png", width=1920, height=1080)
                    st.download_button(
                        label="üì• T·∫£i bi·ªÉu ƒë·ªì PNG",
                        data=img_bytes,
                        file_name=f"graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                        mime="image/png"
                    )
            
            with tab2:
                st.subheader("üö® Danh s√°ch C·∫£nh b√°o")
                
                if not alerts_df.empty:
                    # Th√™m m√†u cho risk score
                    def highlight_risk(val):
                        if val >= 7:
                            return 'background-color: #ff4444; color: white; font-weight: bold'
                        elif val >= 4:
                            return 'background-color: #ffaa00; font-weight: bold'
                        else:
                            return 'background-color: #ffff99'
                    
                    # S·∫Øp x·∫øp theo Risk Score
                    alerts_display = alerts_df.sort_values('Risk_Score', ascending=False).copy()
                    
                    # Format s·ªë ti·ªÅn
                    alerts_display['Tong_tien_display'] = alerts_display['Tong_tien'].apply(
                        lambda x: f"{x:,.0f} VNƒê"
                    )
                    
                    # Hi·ªÉn th·ªã b·∫£ng v·ªõi style
                    styled_df = alerts_display[[
                        'Risk_Score', 'Loai_canh_bao', 'Nguon', 'STT_nhom', 
                        'So_tai_khoan', 'Tong_tien_display', 'Moi_quan_he', 'Chi_tiet'
                    ]].style.applymap(highlight_risk, subset=['Risk_Score'])
                    
                    st.dataframe(styled_df, use_container_width=True, height=400)
                    
                    # Th·ªëng k√™ nhanh
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("T·ªïng c·∫£nh b√°o", len(alerts_df))
                    with col2:
                        high_risk = len(alerts_df[alerts_df['Risk_Score'] >= 7])
                        st.metric("R·ªßi ro cao (‚â•7)", high_risk, delta_color="inverse")
                    with col3:
                        cross_group = len(alerts_df[alerts_df['Loai_canh_bao'] == 'Cross-group'])
                        st.metric("Cross-group", cross_group)
                    with col4:
                        total_alert_money = alerts_df['Tong_tien'].sum()
                        st.metric("T·ªïng ti·ªÅn c·∫£nh b√°o", f"{total_alert_money/1e9:.1f}B VNƒê")
                else:
                    st.info("Kh√¥ng ph√°t hi·ªán c·∫£nh b√°o n√†o")
            
            with tab3:
                st.subheader("üìä Th·ªëng k√™ t·ªïng quan")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Top 10 Ngu·ªìn ti·ªÅn l·ªõn nh·∫•t**")
                    top_sources = edges_df.groupby('Nguon')['Tong_tien'].sum().sort_values(ascending=False).head(10)
                    st.dataframe(top_sources.apply(lambda x: f"{x:,.0f} VNƒê"))
                    
                    st.markdown("**Top 10 T√†i kho·∫£n nh·∫≠n nhi·ªÅu nh·∫•t**")
                    top_accounts = edges_df.groupby('Ten_dich')['Tong_tien'].sum().sort_values(ascending=False).head(10)
                    st.dataframe(top_accounts.apply(lambda x: f"{x:,.0f} VNƒê"))
                
                with col2:
                    st.markdown("**Top 10 T√†i kho·∫£n trung gian (Betweenness)**")
                    betweenness_list = [(G.nodes[n]['label'], metrics[n]['betweenness']) 
                                       for n in G.nodes() if G.nodes[n]['type'] == 'TAIKHOAN']
                    betweenness_list.sort(key=lambda x: x[1], reverse=True)
                    st.dataframe(pd.DataFrame(betweenness_list[:10], 
                                            columns=['T√†i kho·∫£n', 'Betweenness Score']))
                    
                    st.markdown("**Ph√¢n b·ªë Community**")
                    community_dist = pd.Series([metrics[n]['community'] for n in G.nodes()]).value_counts()
                    st.bar_chart(community_dist)
                
                # Network statistics
                st.markdown("---")
                st.markdown("**Ch·ªâ s·ªë m·∫°ng**")
                col1, col2, col3, col4, col5 = st.columns(5)
                
                with col1:
                    st.metric("Nodes", G.number_of_nodes())
                with col2:
                    st.metric("Edges", G.number_of_edges())
                with col3:
                    st.metric("T·ªïng ti·ªÅn", f"{edges_df['Tong_tien'].sum()/1e9:.1f}B")
                with col4:
                    components = nx.number_weakly_connected_components(G)
                    st.metric("Connected Components", components)
                with col5:
                    communities_count = len(set(metrics[n]['community'] for n in G.nodes()))
                    st.metric("Communities", communities_count)
            
            with tab4:
                st.subheader("üíæ Xu·∫•t d·ªØ li·ªáu")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**üìä Xu·∫•t Excel (T·∫•t c·∫£ sheets)**")
                    excel_data = export_to_excel(edges_df, alerts_df, metrics, G)
                    
                    st.download_button(
                        label="üì• T·∫£i file Excel ƒë·∫ßy ƒë·ªß",
                        data=excel_data,
                        file_name=f"money_flow_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                    
                    st.info("File Excel bao g·ªìm 4 sheets:\n- **Edges**: Danh s√°ch c·∫°nh\n- **Alerts**: C·∫£nh b√°o c√≥ Risk Score\n- **Nodes**: Th√¥ng tin nodes\n- **Summary**: T√≥m t·∫Øt")
                
                with col2:
                    st.markdown("**üìÑ Xu·∫•t CSV ri√™ng l·∫ª**")
                    
                    # Edges CSV
                    edges_csv = edges_df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        label="üì• T·∫£i Edges.csv",
                        data=edges_csv,
                        file_name=f"edges_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                    
                    # Alerts CSV
                    if not alerts_df.empty:
                        alerts_csv = alerts_df.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            label="üì• T·∫£i Alerts.csv",
                            data=alerts_csv,
                            file_name=f"alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                
                st.markdown("---")
                
                # Preview data
                with st.expander("üëÅÔ∏è Xem tr∆∞·ªõc d·ªØ li·ªáu Edges"):
                    st.dataframe(edges_df.head(20), use_container_width=True)
                
                if not alerts_df.empty:
                    with st.expander("üëÅÔ∏è Xem tr∆∞·ªõc d·ªØ li·ªáu Alerts"):
                        st.dataframe(alerts_df.head(20), use_container_width=True)
        
        except Exception as e:
            st.error(f"‚ùå L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {str(e)}")
            st.exception(e)
    
    else:
        # Demo mode ho·∫∑c h∆∞·ªõng d·∫´n
        st.info("üëÜ Vui l√≤ng upload file d·ªØ li·ªáu ·ªü sidebar ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch")
        
        with st.expander("üìñ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng"):
            st.markdown("""
            ### C√°ch s·ª≠ d·ª•ng tool:
            
            1. **Upload file lu·ªìng ti·ªÅn** (b·∫Øt bu·ªôc):
               - C·ªôt c·∫ßn c√≥: `Nguoi_nop`, `Tai_khoan`, `So_tien`, `NoP_Rut`, `Tu_chuyen_khoan`, `So_lenh`, `Ten_nha_dau_tu`, `CTCK`
            
            2. **Upload file nh√≥m danh t√≠nh** (t√πy ch·ªçn):
               - C·ªôt c·∫ßn c√≥: `STT_nhom`, `Ma_TK`, `Ten`, `Moi_quan_he_voi_cong_ty`
            
            3. **C·∫•u h√¨nh b·ªô l·ªçc**:
               - Lo·∫°i tr·ª´ giao d·ªãch n·ªôi b·ªô
               - Ch·ªçn lo·∫°i giao d·ªãch (N·ªôp/R√∫t/C·∫£ hai)
               - ƒê·∫∑t ng∆∞·ª°ng ti·ªÅn t·ªëi thi·ªÉu
            
            4. **Ph√¢n t√≠ch k·∫øt qu·∫£**:
               - **Tab Bi·ªÉu ƒë·ªì**: Xem m·∫°ng quan h·ªá t∆∞∆°ng t√°c, t·∫£i HTML/PNG
               - **Tab C·∫£nh b√°o**: Danh s√°ch c·∫£nh b√°o c√≥ Risk Score (0-10)
               - **Tab Th·ªëng k√™**: Top ngu·ªìn ti·ªÅn, t√†i kho·∫£n trung gian, community
               - **Tab Xu·∫•t d·ªØ li·ªáu**: T·∫£i Excel ƒë·∫ßy ƒë·ªß ho·∫∑c CSV ri√™ng l·∫ª
            
            ### √ù nghƒ©a Risk Score:
            - **0-3**: R·ªßi ro th·∫•p (m√†u v√†ng nh·∫°t)
            - **4-6**: R·ªßi ro trung b√¨nh (m√†u cam)
            - **7-10**: R·ªßi ro cao (m√†u ƒë·ªè, c·∫ßn ki·ªÉm tra k·ªπ)
            
            ### C√°c lo·∫°i c·∫£nh b√°o:
            - **Same-group**: C√πng ngu·ªìn ti·ªÅn trong c√πng nh√≥m danh t√≠nh (li√™n h·ªá r·∫•t m·∫°nh)
            - **Cross-group**: C√πng ngu·ªìn ti·ªÅn xuy√™n nhi·ªÅu nh√≥m (c·∫£nh b√°o nghi√™m tr·ªçng)
            - **Trung gian**: T√†i kho·∫£n c√≥ vai tr√≤ trung gian chuy·ªÉn ti·ªÅn
            """)
        
        with st.expander("üß™ Ch·∫°y v·ªõi d·ªØ li·ªáu m·∫´u"):
            st.markdown("""
            D·ªØ li·ªáu m·∫´u ƒë∆∞·ª£c t√≠ch h·ª£p s·∫µn ƒë·ªÉ demo. Tuy nhi√™n, ƒë·ªÉ s·ª≠ d·ª•ng ƒë·∫ßy ƒë·ªß t√≠nh nƒÉng,
            vui l√≤ng upload file d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa b·∫°n.
            """)

if __name__ == "__main__":
    main()
